---
title: "Chapter 9 Support Vector Machines"
author: "Hongyun Wang"
date: "`r gsub(' 0', ' ', format(Sys.Date(), format='%B %d, %Y'))`"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align="center", fig.height=6, fig.width=6,
                      echo=TRUE, warning=FALSE, message=FALSE,
                      tidy=TRUE, tidy.opts=list(width.cutoff=120))
```

# 1.
## 1.(a)
```{r }
x1 = -10:10
x2 = 1 + 3 * x1
plot(x1, x2, xlim=c(-10,10), ylim=c(-30, 30), type = "l", col = "red")
text(-5, 10, expression(paste("1 + 3", italic(X[1]), " - ", italic(X[2])," < 0")) ,  col = "red")
text(5, -10, expression(paste("1 + 3", italic(X[1]), " - ", italic(X[2])," > 0")), col = "red")
```

## 1.(b)
```{r }
x1 = -10:10
x2 = 1 - x1/2
plot(x1, x2, xlim=c(-10,10), ylim=c(-30, 30), type = "l", col = "blue")
text(0, -15, expression(paste("-2 +", italic(X[1]), " + 2 ", italic(X[2])," < 0")),  col = "blue")
text(0, 15, expression(paste("-2 +", italic(X[1]), " + 2 ", italic(X[2])," > 0")),  col = "blue")
```

# 2.
$(1 + X_{1})^2 + (2 - X_{2})^2 = 4$ is a circle with radius 2 and center (-1, 2).

## 2.(a)
```{r}
radius = 2
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), asp = 1, xlab = "X1", ylab = "X2")
symbols(-1, 2, circles = radius, add = TRUE, inches = FALSE)
```

## 2.(b)
```{r}
radius = 2
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), asp = 1, xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), circles = c(radius), add = TRUE, inches = FALSE)
text(-1, 2, "< 4", col="red")
text(-4, 2, "> 4", col="blue")
text(-1, -0.5, "> 4", col="blue")
text(2, 2, "> 4", col="blue")
text(-1, 4.5, "> 4", col="blue")
```

## 2.(c)
```{r}
radius = 2
plot(NA, NA, type = "n", xlim = c(-4, 4), ylim = c(-1, 8), asp = 1, xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), circles = c(radius), add = TRUE, inches = FALSE)
text(-1, 2, "< 4", col="red")
text(-4, 2, "> 4", col="blue")
points(c(0, -1, 2, 3), c(0, 1, 2, 8), col = c("blue", "red", "blue", "blue"), pch=16)
```

## 2.(d)
Expand the formula below,  
$$(1 + X_{1})^2 + (2 - X_{2})^2 > 4$$
then we get  
$$4 + 2X_1 - 4X_2 +X_{1}^2 + X_{2}^2 > 0$$
which is linear in terms of $X_1$, $X_2$, $X_{1}^2$ and $X_{2}^2$.

#3.
## 3.(a)
```{r}
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c(rep("red", 4), rep("blue", 3))
plot(x1, x2, col = colors, pch=16, asp=1, xlim = c(0, 5), ylim = c(0, 5))
text(x1, x2, 1:7, pos = 4)
```

## 3.(b)
According to the scattorplot, it is easy to see observations #2 (2,2), #3 (4,4), #5 (2,1), #6 (4,3) are the support vectors. And the line passing observations 2 and 3 and the line passing observations 5 and 6 are parallel and both the lines have slope $b = \frac {4-2} {4-2} = \frac {3-1} {4-2} = 1$. Then the maximal boundary is a line in the middle of those two lines with the same slope $b=1$. The line also pass $(X_1, X_2) = (2, \frac{2+1}{2}) = (2,1.5)$ and $(X_1, X_2) = (4, \frac{4+3}{2}) = (4,3.5)$, then the intercept is $a = X_2 - bX_1 = 1.5 - 1 \times 2 = 3.5 - 1 \times 4 = -0.5$.
```{r}
plot(x1, x2, col = colors, pch=16, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5,1)

```

## 3.(c)
$0.5 - X_1 + X_2 > 0$  Classfify to Red,
$0.5 - X_1 + X_2 < 0$  Classfify to Blue.

## 3.(d)
```{r}
plot(x1, x2, col = colors, pch=16, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1, lwd=2)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
```

## 3.(e)
There are 4 support vectors:observations #2 (2,2), #3 (4,4), #5 (2,1), #6 (4,3).
```{r}
plot(x1, x2, col = colors, pch=16, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1, lwd=2)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
arrows(2,1,1.75,1.25, code=2)
arrows(2,2,2.25,1.75)
arrows(4,3,3.75,3.25)
arrows(4,4,4.25,3.75)

```

## 3.(f)
A slight movement of observation #7 (4,1) would not affect the maximal margin hyperplane as long as its movement is outside of the margin for the maximal margin hyperplane.

## 3.(g)
```{r}
plot(x1, x2, col = colors, pch=16, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.6, 1.1)
```

$0.6 - 1.1X_1 + X_2 = 0$

## 3.(h)
```{r}
plot(x1, x2, col = colors, pch=16, xlim = c(0, 5), ylim = c(0, 5))
points(3.5, 1.5, pch=16, col="red")
text(3.5,1.5,pos=4,"new")
```

# 4.
```{r}
set.seed(1000)
x1 = rnorm(100)
x2 = 2 * x1^2 - 3 + rnorm(100)
class1 = sample(100, 50)
x2[class1] = x2[class1] + 2
x2[-class1] = x2[-class1] - 2
# Plot using different colors
plot(x1[class1], x2[class1], pch=4, col="red", xlim=c(-3,4),ylim=c(-10, 15), xlab="X1", ylab="X2")
points(x1[-class1], x2[-class1], col="blue")
```

```{r}
set.seed(300)
z = rep(0, 100)
z[class1] = 1
# Take 25 observations each from train and -train
final.train = c(sample(class1, 25), sample(setdiff(1:100, class1), 25))
data.train = data.frame(x1=x1[final.train], x2=x2[final.train], z=as.factor(z[final.train]))
data.test = data.frame(x1=x1[-final.train], x2=x2[-final.train], z=as.factor(z[-final.train]))
# Plot using different colors
plot(subset(data.train, z == 1)[, c(1:2)], pch=4, col="red", xlim=c(-3,4),ylim=c(-10, 15), xlab="X1", ylab="X2")
points(subset(data.train, z == 0)[, c(1:2)], col="blue")
points(subset(data.test, z == 1)[, c(1:2)], pch=3,col="purple")
points(subset(data.test, z == 0)[, c(1:2)], pch=16,col="cyan")
legend("topleft", inset=c(0.001, 0.001), pch=c(4,1,3,16), legend=c("Train Class 1", "Train Class 0", "Test Class 1", "Test Class 0"),
       col=c("red","blue","purple","cyan"), cex=0.8, pt.cex=1, bty="n")
```

```{r fig.height=6, fig.width=10, fig.cap="Train data, linear basis kernel"}
suppressMessages(library(e1071))
svm.linear = svm(z~., data=data.train, kernel="linear", cost=10)
plot(svm.linear, data.train)
table(z[final.train], predict(svm.linear, data.train))
```
```{r fig.height=6, fig.width=10, fig.cap="Train data, polynomial basis kernel"}
svm.poly = svm(z~., data=data.train, kernel="polynomial", cost=10)
plot(svm.poly, data.train)
table(z[final.train], predict(svm.poly, data.train))
```

```{r fig.height=6, fig.width=10, fig.cap="Train data, radial basis kernel"}
svm.radial = svm(z~., data=data.train, kernel="radial", gamma=1, cost=10)
plot(svm.radial, data.train)
table(z[final.train], predict(svm.radial, data.train))
```

Test data

```{r fig.height=6, fig.width=10, fig.cap="Test data, linear basis kernel"}
plot(svm.linear, data.test)
table(z[-final.train], predict(svm.linear, data.test))
```

```{r fig.height=6, fig.width=10, fig.cap="Test data, polynomial basis kernel"}
plot(svm.poly, data.test)
table(z[-final.train], predict(svm.poly, data.test))
```

```{r fig.height=6, fig.width=10, fig.cap="Test data, radial basis kernel"}
plot(svm.radial, data.test)
table(z[-final.train], predict(svm.radial, data.test))
```

# 5.
## 5.(a)
```{r}
set.seed(1001)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
```

## 5.(b)
```{r}
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2", pch = "+")
points(x1[y == 1], x2[y == 1], col = "blue", pch = 4)
```
The plot clearly shows non-linear decision boundary.

## 5.(c)
```{r}
lm.fit = glm(y ~ x1 + x2, family = binomial)
summary(lm.fit)
```
Variable **x1** is significant and **x2** is insignificant for predicting **y** when setting $\alpha = 0.05$.

## 5.(d)
```{r}
data = data.frame(x1 = x1, x2 = x2, y = y)
lm.prob = predict(lm.fit, data, type = "response")
lm.pred = ifelse(lm.prob > 0.5, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
```
The decision boundary is linear as seen in the figure above.

## 5.(e)
Non-linear functions (squares, product interaction) of *X1* and *X2* are used to fit the model.
```{r}
lm.fit2 = glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2), data = data, family = binomial)
summary(lm.fit2)
```

## 5.(f)
```{r}
lm.prob = predict(lm.fit2, data, type = "response")
lm.pred = ifelse(lm.prob > 0.5, 1, 0)
data.pos = data[lm.pred == 1, ]
data.neg = data[lm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
```
The decision boundary is obviously no-linear as seen in the figure.

## 5.(g)
```{r}
svm.fit = svm(as.factor(y) ~ x1 + x2, data, kernel = "linear", cost = 0.1)
svm.pred = predict(svm.fit, data)
data.pos = data[svm.pred == 1, ]
data.neg = data[svm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
```
A linear basis kernel, even with low cost **0.1** fails to find non-linear decision boundary.

## 5.(h)
```{r}
svm.fit = svm(as.factor(y) ~ x1 + x2, data, kernel="radial", gamma = 1, cost=0.1)
svm.pred = predict(svm.fit, data)
data.pos = data[svm.pred == 1, ]
data.neg = data[svm.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
```
When radial basis kernel is used, the predicted decision boundary closely resembles the true decision boundary.

## 5.(i)
This experiment enforces the idea that SVMs with non-linear kernel are extremely powerful in finding non-linear boundary. Both, logistic regression with linear terms and SVMs with linear kernels fail to find the decision boundary. Adding non-linear squares and product terms to logistic regression seems to give them same power as radial-basis kernels. However, there is some manual efforts and tuning involved in picking the right non-linear terms. This effort can become prohibitive with large number of features. Radial basis kernels, on the other hand, only require tuning of one parameter - gamma - which can be easily done using cross-validation.

#6.
## 6.(a)
```{r}
set.seed(1)
x=matrix(rnorm(40*2), ncol=2)
y=sample(c(-1,1), 40, rep=T)
x[y==1,]=x[y==1,] + 2.5
plot(x, col=(3-y), pch=19)
dat=data.frame(x=x, y=as.factor(y))
```

## 6.(b)
```{r}
set.seed(1)
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100, 1000)))
summary(tune.out)
bestmod=tune.out$best.model
summary(bestmod)

plot(bestmod, dat)
table(dat$y, predict(bestmod, dat))
```
We see that **cost=0.1** results in the lowest cross-validation error rate. When set **cost=0.1**, there are **`r length(bestmod$index)`** support vectors, which are corresponding to a much wider margin. **1** out of **40** train observations is misclassfied.  

```{r}
svmfit=svm(y~., data=dat, kernel="linear", cost=1e+5,scale=FALSE)
plot(svmfit, dat)
table(dat$y, predict(svmfit, dat))
```

When set a very large value of **cost=1e+5**, there are only **`r length(svmfit$index)`** support vectors, which are corresponding to a much narrower margin. **1** out of **40** train observations is misclassfied.

## 6.(c)
```{r}
test_err_cost <- function (traindat, cost1, n)  {
  svmfit_train=svm(y~., data=traindat, kernel="linear", cost=cost1,scale=FALSE) # data is fixed as train data, but cost changes.
  table1 = table(traindat$y, predict(svmfit_train,traindat))
  err_train = round((1 - sum(diag(table1))/sum(table1)),4)
  sv_no_train = length(svmfit_train$index)
  
  err_test <- rep(NA, n)
  for (i in 1:n) {
    xtest=matrix(rnorm(n*2), ncol=2)
    ytest=sample(c(-1,1), n, rep=TRUE)
    xtest[ytest==1,]=xtest[ytest==1,] + 2.5
    testdat=data.frame(x=xtest, y=as.factor(ytest))
    
    err_test[i] = 1-sum(diag(table(testdat$y, predict(svmfit_train,testdat))))/n
  }
  data.frame(cost.train=cost1, sv.no.train=sv_no_train, error.train=err_train, 
             error.test=round(mean(err_test),4))
}

out=do.call(rbind, lapply(c(0.001, 0.01, 0.1, 1,5,10,100, 1000), test_err_cost, traindat=dat, n=100))
suppressMessages(library(pander))

pander(out, caption="Cost change effects on fitting train and test data", split.table = Inf, justify = rep("right",4))
```

When cost changes from **0.001** to **1000** for train model, the number of support vectors decreases from **32** to **5**. Although the lowest **train error `r min(out$error.train)`** was achieved at several costs, but the lowest **test error `r min(out$error.test)`** was achieved at **cost = 0.1**.

## 6.(d)
From 6(c), we can see overfitting in train data for linear kernel. A large cost corresponding to lower number of support vectors and narrower margins can correctly classify noisy observations thus overfit the train data. A small cost compromises on fitting the train data (higher train errors) but improves on fitting the test data (lower test errors).

#7.
## 7.(a)
```{r}
library(ISLR)
suppressMessages(library(dplyr))

data(Auto)
Auto = mutate(Auto, mpg = ifelse(Auto$mpg > median(Auto$mpg), 1, 0))
Auto$mpg = as.factor(Auto$mpg)
```

## 7.(b)
```{r}
set.seed(1)
tune.out = tune(svm,mpg~.,data=Auto,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100, 1000)))
summary(tune.out)
summary(tune.out$best.model)
```
The lowest cross-validation error **`r round(tune.out$best.performance,4)`** was achieved at **`r paste0("cost=",tune.out$best.parameters)`**

## 7.(c)
```{r}
set.seed(1)
tune.out = tune(svm, mpg~., data = Auto, kernel = "polynomial", 
                ranges = list(cost = c(0.1, 1, 5, 10), degree = c(2, 3, 4)))
summary(tune.out)
summary(tune.out$best.model)
```
When using *polynomial* basis kernel, the lowest cross-validation error **`r round(tune.out$best.performance,4)`** is obtained when **`r paste0("cost=",tune.out$best.parameters[1])`** and **`r paste0("degree=",tune.out$best.parameters[2])`**.

```{r}
set.seed(463)
tune.out = tune(svm, mpg~., data = Auto, kernel = "radial", 
                ranges = list(cost = c(0.1, 1, 5, 10), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
summary(tune.out$best.model)

```
When using *radial* basis kernel, the lowest cross-validation error **`r round(tune.out$best.performance,4)`** is obtained when **`r paste0("cost=",tune.out$best.parameters[1])`** and **`r paste0("gamma=",tune.out$best.parameters[2])`**.

## 7.(d)
```{r fig.height=5,fig.width=10}
svmfit.linear = svm(mpg~., data=Auto, kernel="linear", cost=1)
svmfit.poly = svm(mpg~., data=Auto, kernel="polynomial", cost=10, degree=2)
svmfit.radial = svm(mpg~., data=Auto, kernel="radial", cost=5, gamma=0.1)

plot_pairs <- function(svmfit){
  par(mfrow=c(2,2))
  for (i in vars[1:(length(vars)-1)]) {
    for (j in vars[(which(vars == i)+1):length(vars)]) {
      plot(svmfit, Auto, as.formula(paste0(i,"~",j)))
    }
  }
}

vars = names(Auto)[!(names(Auto) %in% c("mpg","name"))]

#plot_pairs(svmfit.linear)
#plot_pairs(svmfit.poly)
#plot_pairs(svmfit.radial)
```

# 8.
## 8.(a)
```{r}
set.seed(1)
train = sample(nrow(OJ), 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
```

## 8.(b)

```{r}
svmfit.linear = svm(Purchase~., data=OJ.train, kernel = "linear", cost = 0.01)
summary(svmfit.linear)
```
Support vector classifier creates **432** support vectors out of **800** training points. Out of these, **217** belong to level *CH* and remaining **215** belong to level *MM*.

## 8.(c)
```{r}
(table1=table(OJ.train$Purchase, predict(svmfit.linear, OJ.train)))
(err_train=round((1-sum(diag(table1))/sum(table1)),4))

(table2=table(OJ.test$Purchase, predict(svmfit.linear, OJ.test)))
(err_test=round((1-sum(diag(table2))/sum(table2)),4))
```
The training error rate is **`r paste0(round(err_train,3)*100,"%")`** and the test error rate is **`r paste0(round(err_test,3)*100,"%")`**

## 8.(d)
```{r}
set.seed(100)
tune.out = tune(svm, Purchase~., data = OJ.train, kernel = "linear", ranges = list(cost = 10^seq(-2,1,by=0.25)))
summary(tune.out)
```
Tunning shows that optimal cost is **`r round(tune.out$best.parameters,4)`**.

## 8.(e)
```{r}
(table1=table(OJ.train$Purchase, predict(tune.out$best.model, OJ.train)))
(err_train=round((1-sum(diag(table1))/sum(table1)),4))

(table2=table(OJ.test$Purchase, predict(tune.out$best.model, OJ.test)))
(err_test=round((1-sum(diag(table2))/sum(table2)),4))
```
The training error rate is **`r paste0(round(err_train,3)*100,"%")`** and the test error rate is **`r paste0(round(err_test,3)*100,"%")`**

## 8.(f)
```{r}
svmfit.radial = svm(Purchase~., data=OJ.train, kernel = "radial")
summary(svmfit.radial)
(table1=table(OJ.train$Purchase, predict(svmfit.radial, OJ.train)))
(err_train=round((1-sum(diag(table1))/sum(table1)),4))
(table2=table(OJ.test$Purchase, predict(svmfit.radial, OJ.test)))
(err_test=round((1-sum(diag(table2))/sum(table2)),4))

set.seed(100)
tune.out = tune(svm, Purchase~., data = OJ.train, kernel = "radial", ranges = list(cost = 10^seq(-2,1,by=0.25)))
summary(tune.out)
(table1=table(OJ.train$Purchase, predict(tune.out$best.model, OJ.train)))
(err_train=round((1-sum(diag(table1))/sum(table1)),4))
(table2=table(OJ.test$Purchase, predict(tune.out$best.model, OJ.test)))
(err_test=round((1-sum(diag(table2))/sum(table2)),4))
```
When using *radial* basis kernel, tunning didn't change training error rate **`r paste0(round(err_train,3)*100,"%")`** but slightly decreased test error rate to **`r paste0(round(err_test,3)*100,"%")`**.

## 8.(g)
```{r}
svmfit.polynomial = svm(Purchase~., data=OJ.train, kernel = "polynomial",degree=2)
summary(svmfit.polynomial)
(table1=table(OJ.train$Purchase, predict(svmfit.polynomial, OJ.train)))
(err_train=round((1-sum(diag(table1))/sum(table1)),4))
(table2=table(OJ.test$Purchase, predict(svmfit.polynomial, OJ.test)))
(err_test=round((1-sum(diag(table2))/sum(table2)),4))

set.seed(100)
tune.out = tune(svm, Purchase~., data = OJ.train, kernel = "polynomial", degree=2, ranges = list(cost = 10^seq(-2,1,by=0.25)))
summary(tune.out)
(table1=table(OJ.train$Purchase, predict(tune.out$best.model, OJ.train)))
(err_train=round((1-sum(diag(table1))/sum(table1)),4))
(table2=table(OJ.test$Purchase, predict(tune.out$best.model, OJ.test)))
(err_test=round((1-sum(diag(table2))/sum(table2)),4))
```
When using *polynomial* basis kernel, tunning slightly decreased training error rate to **`r paste0(round(err_train,3)*100,"%")`** and test error rate to **`r paste0(round(err_test,3)*100,"%")`**.

## 8.(h)
Overall, radial basis kernel semms to generate lowest classification error rate for both training and test data.
